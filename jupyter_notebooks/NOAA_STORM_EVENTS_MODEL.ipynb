{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lastEditStatus": {
   "notebookId": "lmdf6l3awsm4733tfb7s",
   "authorId": "4420909122518",
   "authorName": "BEETLE",
   "authorEmail": "anushmita.sky@gmail.com",
   "sessionId": "cb16c3ea-f2d0-4a9f-93fb-b0ebca65e1a3",
   "lastEditTime": 1763160618911
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24632bec-be24-4bc0-9fa4-f4100088adb7",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## NATURAL DISASTER PREDICTION USING DECISION TREE"
  },
  {
   "cell_type": "code",
   "id": "9b7e3b07-541b-4e5e-b0ce-29656cefbf8f",
   "metadata": {
    "language": "python",
    "name": "Setup_and_Data_Loading"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col, udf, count, sum as sf_sum, avg, max as sf_max\nfrom snowflake.snowpark.functions import concat, lpad, lit\nfrom snowflake.snowpark.types import FloatType, StringType\n\n\n\nsession = get_active_session()\nsession.sql(\"USE DATABASE NOAA_STORM_EVENTS_2025\").collect()\nsession.sql(\"USE SCHEMA PUBLIC\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a4794cf-d1c4-430e-8525-9438307ba273",
   "metadata": {
    "language": "python",
    "name": "Data_Preprocessing"
   },
   "outputs": [],
   "source": "df = session.table(\"STORM_EVENTS_DETAILS\") # loads the raw NOAA storm events table\n\ndf = df.dropna(subset=['STATE', 'EVENT_TYPE', 'STATE_FIPS', 'CZ_FIPS']) # ensure valid FIPS codes\n\n# keep FIPS and other relevant columns\ndf = df.select([\n    'STATE_FIPS',\n    'CZ_FIPS',  \n    'STATE', \n    'YEAR', \n    'MONTH_NAME', \n    'BEGIN_LAT', \n    'BEGIN_LON',\n    'EVENT_TYPE', \n    'MAGNITUDE', \n    'INJURIES_DIRECT', \n    'DEATHS_DIRECT',\n    'DAMAGE_PROPERTY', \n    'DAMAGE_CROPS'\n])\n\n# ===== CREATE FULL 5-DIGIT COUNTY FIPS CODE =====\n# creates a new column `FIPS_CODE` by combining STATE_FIPS + CZ_FIPS\ndf = df.with_column(\n    \"FIPS_CODE\",\n    concat(\n        lpad(col(\"STATE_FIPS\").cast(\"STRING\"), lit(2), lit(\"0\")),\n        lpad(col(\"CZ_FIPS\").cast(\"STRING\"), lit(3), lit(\"0\"))\n    )\n)\n\nprint(f\" Created 5-digit FIPS codes for {df.count()} storm events\")\n\n# damage parsing UDF\n# Converts damage strings like \"50K\" or \"2.5M\" into actual numbers\n# \"50K\" → 50,000\n# \"2.5M\" → 2,500,000\n\ndef parse_damage(val):\n    if val is None:\n        return 0.0\n    val = str(val).upper().replace('$','').strip()\n    try:\n        if 'K' in val:\n            return float(val.replace('K','')) * 1_000\n        elif 'M' in val:\n            return float(val.replace('M','')) * 1_000_000\n        elif 'B' in val:\n            return float(val.replace('B','')) * 1_000_000_000\n        else:\n            return float(val)\n    except ValueError:\n        return 0.0\n\nparse_damage_udf = udf(func=parse_damage, return_type=FloatType(), input_types=[StringType()])\ndf = df.with_column(\"DAMAGE_PROPERTY\", parse_damage_udf(col(\"DAMAGE_PROPERTY\"))) \\\n       .with_column(\"DAMAGE_CROPS\", parse_damage_udf(col(\"DAMAGE_CROPS\")))\n\n# now DAMAGE_PROPERTY and DAMAGE_CROPS are numbers we can sum/average\n\n# event categorization - Groups 48+ different event types into 13 broader categories\ndef categorize_event(event):\n    if event is None:\n        return 'Other'\n    event = str(event).title().strip()\n    if event in ['Flood', 'Flash Flood', 'Coastal Flood']:\n        return 'Flood'\n    elif event in ['Tornado']:\n        return 'Tornado'\n    elif event in ['Thunderstorm Wind', 'Strong Wind', 'High Wind', \n                   'Marine Thunderstorm Wind', 'Marine High Wind']:\n        return 'Wind'\n    elif event in ['Hail']:\n        return 'Hail'\n    elif event in ['Lightning']:\n        return 'Lightning'\n    elif event in ['Blizzard', 'Winter Storm', 'Winter Weather', 'Heavy Snow', \n                   'Sleet', 'Ice Storm', 'Lake-Effect Snow']:\n        return 'Snow/Ice'\n    elif event in ['Drought', 'Heat', 'Excessive Heat']:\n        return 'Heat/Drought'\n    elif event in ['Heavy Rain', 'Debris Flow']:\n        return 'Heavy Rain'\n    elif event in ['Cold/Wind Chill', 'Extreme Cold/Wind Chill', 'Frost/Freeze']:\n        return 'Cold'\n    elif event in ['Wildfire']:\n        return 'Wildfire'\n    elif event in ['Dust Storm']:\n        return 'Dust Storm'\n    elif event in ['High Surf', 'Tsunami', 'Hurricane', 'Tropical Storm']:\n        return 'Coastal/Marine'\n    else:\n        return 'Other'\n\n# create new column `EVENT_GROUP` with simplified categories\ncategorize_event_udf = udf(func=categorize_event, return_type=StringType(), input_types=[StringType()])\ndf = df.with_column(\"EVENT_GROUP\", categorize_event_udf(col(\"EVENT_TYPE\")))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d958898-6a68-45e3-8a66-dcd699dc7679",
   "metadata": {
    "language": "python",
    "name": "Save_Fips_Code"
   },
   "outputs": [],
   "source": "# convert to pandas for modeling\ndf_pandas = df.to_pandas()\n\n# ===== STORE FIPS BEFORE ENCODING =====\nfips_codes = df_pandas['FIPS_CODE'].copy() ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a16ae70e-d1d0-4011-b8f1-2975a128859d",
   "metadata": {
    "language": "python",
    "name": "Train_Model"
   },
   "outputs": [],
   "source": "# encode and train model \ndf_model = pd.get_dummies(df_pandas, columns=['STATE', 'MONTH_NAME'], drop_first=True)\nX = df_model.drop(columns=['EVENT_TYPE', 'EVENT_GROUP', 'STATE_FIPS', 'CZ_FIPS', 'FIPS_CODE'])\ny = df_model['EVENT_GROUP']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nmodel = DecisionTreeClassifier(\n    max_depth=6,\n    class_weight='balanced',\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n\n# Evaluate model\ny_pred = model.predict(X_test)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c790f40-2ced-443a-94d7-b2ac7acebcc3",
   "metadata": {
    "language": "python",
    "name": "Calculate_Severity_Score"
   },
   "outputs": [],
   "source": "# ===== CREATE COUNTY-LEVEL RISK FEATURES =====\n\n# Get predictions for all data\ndf_pandas['PREDICTED_EVENT'] = model.predict(X)\n\n# Get prediction probabilities for severity scoring\npred_proba = model.predict_proba(X)\n\n# Create a severity score (max probability of high-risk events)\nhigh_risk_events = ['Tornado', 'Flood', 'Wind', 'Snow/Ice']\nhigh_risk_indices = [i for i, event in enumerate(model.classes_) if event in high_risk_events]\n\n# gets probability scores for each category\n# example: Event A might be 70% Wind, 20% Hail, 10% Other\n# 'PREDICTED_SEVERITY`: Takes the MAX probability for high-risk events\n# this becomes our severity score(0 to 1)\n# severity score: probability of being a natural disaster\n\nif high_risk_indices:\n    df_pandas['PREDICTED_SEVERITY'] = pred_proba[:, high_risk_indices].max(axis=1)\nelse:\n    df_pandas['PREDICTED_SEVERITY'] = pred_proba.max(axis=1)\n\n# add FIPS back\ndf_pandas['FIPS_CODE'] = fips_codes",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "975bb689-4bff-4736-9338-c2b369d04e69",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "# ===== DISPLAY INDIVIDUAL EVENT PREDICTIONS =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"INDIVIDUAL STORM EVENT PREDICTIONS\")\nprint(\"=\"*80)\n\n# Show sample of predictions\ndisplay_df = df_pandas[[\n    'FIPS_CODE', \n    'STATE',\n    'EVENT_TYPE', \n    'PREDICTED_EVENT',\n    'PREDICTED_SEVERITY',\n    'DAMAGE_PROPERTY',\n    'INJURIES_DIRECT',\n    'DEATHS_DIRECT'\n]].copy()\n\n# Round severity for readability\ndisplay_df['PREDICTED_SEVERITY'] = display_df['PREDICTED_SEVERITY'].round(3)\n\nprint(\"\\n Sample of 20 Storm Events with Predictions:\")\nprint(display_df.head(20).to_string(index=False))\n\nprint(\"\\n Distribution of Predicted Severity Scores:\")\nprint(display_df['PREDICTED_SEVERITY'].describe())\n\n# Show highest severity events\nprint(\"\\n TOP 10 MOST SEVERE EVENTS (Highest Predicted Severity):\")\ntop_severe = display_df.nlargest(10, 'PREDICTED_SEVERITY')\nprint(top_severe.to_string(index=False))\n\n# Show lowest severity events\nprint(\"\\n TOP 10 LEAST SEVERE EVENTS (Lowest Predicted Severity):\")\nlow_severe = display_df.nsmallest(10, 'PREDICTED_SEVERITY')\nprint(low_severe.to_string(index=False))\n\n# Comparison: Actual vs Predicted Event Type\nprint(\"\\n EVENT TYPE COMPARISON (Sample):\")\ncomparison_df = df_pandas[[\n    'EVENT_TYPE',\n    'EVENT_GROUP', \n    'PREDICTED_EVENT',\n    'PREDICTED_SEVERITY',\n    'DAMAGE_PROPERTY'\n]].head(15)\ncomparison_df['PREDICTED_SEVERITY'] = comparison_df['PREDICTED_SEVERITY'].round(3)\nprint(comparison_df.to_string(index=False))\n\n# Check prediction accuracy\ncorrect_predictions = (df_pandas['EVENT_GROUP'] == df_pandas['PREDICTED_EVENT']).sum()\ntotal_predictions = len(df_pandas)\naccuracy = correct_predictions / total_predictions\nprint(f\"\\n Model Accuracy on All Events: {accuracy:.2%} ({correct_predictions}/{total_predictions})\")\n\nprint(\"=\"*80 + \"\\n\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6123f74e-630f-429d-918a-a8d20e5a5283",
   "metadata": {
    "language": "python",
    "name": "Aggregate_by_County"
   },
   "outputs": [],
   "source": "# ===== AGGREGATE BY COUNTY (FIPS) =====\n# group all events by county \n# for each county, calculate:\n# - **Mean severity**: Average of all PREDICTED_SEVERITY scores\n# - **Max severity**: Worst storm that county experienced\n# - **Std severity**: How variable the storms were\n# - **Total injuries/deaths**: Sum across all events\n# - **Total damage**: Sum of all property/crop damage\n# - **Event count**: How many storms hit that county\n\ncounty_risk = df_pandas.groupby('FIPS_CODE').agg({\n    'PREDICTED_SEVERITY': ['mean', 'max', 'std'],  # Severity metrics\n    'INJURIES_DIRECT': 'sum',                       # Total injuries\n    'DEATHS_DIRECT': 'sum',                         # Total deaths\n    'DAMAGE_PROPERTY': 'sum',                       # Total property damage\n    'DAMAGE_CROPS': 'sum',                          # Total crop damage\n    'EVENT_TYPE': 'count'                           # Event frequency\n}).reset_index()\n\n# 8 storm features!\ncounty_risk.columns = [\n    'FIPS_CODE',\n    'STORM_SEVERITY_MEAN',\n    'STORM_SEVERITY_MAX',\n    'STORM_SEVERITY_STD',\n    'STORM_INJURIES_TOTAL',\n    'STORM_DEATHS_TOTAL',\n    'STORM_DAMAGE_PROPERTY_TOTAL',\n    'STORM_DAMAGE_CROPS_TOTAL',\n    'STORM_EVENT_COUNT'\n]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa0aed19-e009-4a3e-bbf2-7cf81ea77734",
   "metadata": {
    "language": "python",
    "name": "Save_to_Snowflake"
   },
   "outputs": [],
   "source": "# Handle NaN in std (counties with only 1 event)\ncounty_risk['STORM_SEVERITY_STD'] = county_risk['STORM_SEVERITY_STD'].fillna(0)\ncounty_risk['FIPS_CODE'] = county_risk['FIPS_CODE'].astype(str).str.zfill(5)\n\n# ===== SAVE TO SNOWFLAKE =====\n# Convert back to Snowpark DataFrame\ncounty_risk_sp = session.create_dataframe(county_risk)\n\n# Create or replace the aggregated table\ncounty_risk_sp.write.mode(\"overwrite\").save_as_table(\"STORM_COUNTY_RISK_FEATURES\")\n\nprint(\"✅ County-level storm risk features saved to STORM_COUNTY_RISK_FEATURES\")\nprint(f\"✅ Created {len(county_risk)} county-level risk profiles\")\nprint(\"\\nSample of features created:\")\nprint(county_risk.head())",
   "execution_count": null
  }
 ]
}