{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ap2q4rzlzsxomfb4bjvb",
   "authorId": "1648527949776",
   "authorName": "BOA",
   "authorEmail": "sara.alaidroos@gmail.com",
   "sessionId": "a7e2c7d2-be69-4343-af9d-e70fd0cbfac3",
   "lastEditTime": 1764208686991
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b46c424-a9b8-44f1-a9d8-72a407e0f3b1",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "Selecting Traffic Congestion Data"
  },
  {
   "cell_type": "code",
   "id": "84ec9b05-e7f7-424f-aad6-a7bfeff213fa",
   "metadata": {
    "language": "sql",
    "name": "cell33"
   },
   "outputs": [],
   "source": "SELECT CURRENT_DATABASE(), CURRENT_SCHEMA();\nSHOW DATABASES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bd43b69-78a8-48df-bc70-a20ead480d61",
   "metadata": {
    "language": "sql",
    "name": "cell35"
   },
   "outputs": [],
   "source": "USE DATABASE PROJECT_DB_FINAL;\nUSE SCHEMA PROCESSED_DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c8513e3-8087-41d3-a528-cc8c0a0c0ee1",
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "cell1",
    "codeCollapsed": false
   },
   "source": "SELECT * FROM PROCESSED_DATA.TRAFFIC_CONGESTION_CAUSES LIMIT 5;\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3043839a-08f8-403b-9293-31f8c2b2454d",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CLEANED_TRAFFIC AS\nSELECT \n-- change all to uppercase\n    UPPER(TRIM(COUNTY)) as COUNTY_NAME,\n    UPPER(TRIM(STATE)) as STATE_ABBR,\n    SUM(\"Incident (VHD)\") as TOTAL_INCIDENT_CONGESTION,\n    SUM(\"Weather (VHD)\") as TOTAL_WEATHER_CONGESTION,\n    SUM(\"Workzone (VHD)\") as TOTAL_WORKZONE_CONGESTION,\n    \n    -- Create overall congestion score\n    SUM(\"Incident (VHD)\" + \"Weather (VHD)\" + \"Workzone (VHD)\" + \"Recurrent (VHD)\") as TOTAL_CONGESTION_SCORE\nFROM TRAFFIC_CONGESTION_CAUSES\nWHERE STATE IN ('AL','AR','FL','GA','LA','MS','NC','SC','TX','TN')\nGROUP BY COUNTY_NAME, STATE_ABBR;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96842088-bc6f-4e35-8398-63911ab5557f",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "--view new created table \nSELECT * FROM PROCESSED_DATA.CLEANED_TRAFFIC LIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68a02bf3-74e6-4223-8c50-6f5ccc9e0a90",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Selecting HPSA Data"
  },
  {
   "cell_type": "code",
   "id": "4530e3b6-c67d-48d5-9789-b30ca972791e",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "SELECT * FROM PROCESSED_DATA.HPSA_DATA_FIXED LIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea1f03c1-14ef-4abc-8e74-eb05d642d3f7",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CLEANED_HPSA AS\nSELECT \n    FIPS_CODE,\n    HPSA_SCORE,\n    DESIGNATION_TYPE,\n    HPSA_DESIGNATION_POPULATION,\n    PCT_POPULATION_BELOW_POVERTY,\n    RURAL_STATUS,\n    STATE_ABBR\nFROM HPSA_DATA_FIXED\nWHERE STATE_ABBR IN ('AL','AR','FL','GA','LA','MS','NC','SC','TX','TN')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12c848b2-b6ec-4fad-8377-7f5fa9e8962b",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "--view new table\nSELECT * FROM PROCESSED_DATA.CLEANED_HPSA LIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca3fc264-0c15-4770-b53f-3955c37f6058",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Selecting CDC SVI Data"
  },
  {
   "cell_type": "code",
   "id": "bafdcca2-79e1-4165-9f72-302af1b592b2",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CLEANED_CDC_SVI AS\nSELECT \n     SUBSTRING(FIPS, 1, 5) as FIPS_CODE,\n    -- Key SVI themes (for vulnerability)\n    RPL_THEMES as OVERALL_SVI_SCORE,           -- Overall vulnerability\n    RPL_THEME1 as SOCIOECONOMIC_VULNERABILITY, -- Poverty, unemployment, etc.\n    RPL_THEME2 as HOUSEHOLD_VULNERABILITY,     -- Age 65+, disability, etc.\n    RPL_THEME3 as MINORITY_STATUS,             -- Race, language barriers\n    RPL_THEME4 as HOUSING_TRANSPORT,           -- Housingtype, vehicle access\n    E_TOTPOP as TOTAL_POPULATION   \nFROM CDC_SVI_SOUTH\nWHERE ST_ABBR IN ('AL','AR','FL','GA','LA','MS','NC','SC','TX','TN')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58e8e3ea-da10-4d07-bc3f-f7b1fd4d8b33",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM PROCESSED_DATA.CLEANED_CDC_SVI LIMIT 50",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4ad05a0-c3a9-4b9e-8366-2253a078a047",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "Some issues with CDC Places... plan to drop for  now"
  },
  {
   "cell_type": "markdown",
   "id": "4806aa0e-487b-4328-b7a0-4cffed9cbb9d",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "Selecting from NOAA Storms. The FIPS code is either state (2 numbers) or country (3 numbers) level which makes it difficult to keep consistent with the rest."
  },
  {
   "cell_type": "code",
   "id": "fbb39144-0d92-46bc-9fb5-72590658f2b5",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CLEANED_NOAA AS\nSELECT \n    STATE,\n    STATE_FIPS,\n    CZ_FIPS,\n    LPAD(STATE_FIPS::STRING, 2, '0') || LPAD(CZ_FIPS::STRING, 3, '0') AS         FIPS_CODE,\n    CASE \n        WHEN EVENT_TYPE IN ('Hurricane','Tropical Storm','Flood','Tornado',\n                           'Thunderstorm Wind','Hail','Lightning') THEN EVENT_TYPE\n        ELSE 'Other Severe'\n    END as SEVERE_EVENT_TYPE,\n    COUNT(*) as EVENT_COUNT,\n    SUM(\n        CASE \n            -- Handle 'K' for thousands\n            WHEN UPPER(DAMAGE_PROPERTY) LIKE '%K' THEN \n                TRY_CAST(REPLACE(UPPER(DAMAGE_PROPERTY), 'K', '') AS FLOAT) * 1000\n            -- Handle 'M' for millions  \n            WHEN UPPER(DAMAGE_PROPERTY) LIKE '%M' THEN\n                TRY_CAST(REPLACE(UPPER(DAMAGE_PROPERTY), 'M', '') AS FLOAT) * 1000000\n            -- Handle plain numbers\n            ELSE TRY_CAST(DAMAGE_PROPERTY AS FLOAT)\n        END\n    ) as TOTAL_DAMAGE\nFROM STORM_EVENTS_DETAILS\nWHERE STATE IN ('ALABAMA','ARKANSAS','FLORIDA','GEORGIA',\n               'LOUISIANA','MISSISSIPPI','NORTH CAROLINA','SOUTH CAROLINA',\n               'TENNESSEE','TEXAS')\nGROUP BY \n    STATE,\n    STATE_FIPS,\n    CZ_FIPS,\n    SEVERE_EVENT_TYPE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4045450-49ab-4937-8d52-2b2d03e685be",
   "metadata": {
    "language": "sql",
    "name": "cell21",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM PROCESSED_DATA.CLEANED_NOAA LIMIT 50",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce5d6341-6530-4fa6-91df-99cf0e564552",
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CLEANED_NOAA_AGGREGATED AS\nSELECT \n    FIPS_CODE,\n    STATE,\n    COUNT(*) as TOTAL_EVENT_COUNT,\n    SUM(TOTAL_DAMAGE) as TOTAL_DAMAGE_SUM,\n    -- You might want to keep event type breakdowns in a separate column\n    ARRAY_AGG(SEVERE_EVENT_TYPE) as EVENT_TYPES,\n    ARRAY_AGG(EVENT_COUNT) as EVENT_COUNTS_BY_TYPE\nFROM CLEANED_NOAA\nGROUP BY FIPS_CODE, STATE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56ddce54-3d5c-4c23-b53b-6d7e96650e8b",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1c05a1f-7ea2-4428-9e43-e27162be0856",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_svi = session.table('CLEANED_CDC_SVI')\ndf_svi = df_svi.to_pandas()\ndf_hpsa = session.table('CLEANED_HPSA')\ndf_hpsa = df_hpsa.to_pandas()\ndf_noaa = session.table('CLEANED_NOAA')\ndf_noaa = df_noaa.to_pandas()\ndf_traffic = session.table('CLEANED_TRAFFIC')\ndf_traffic = df_traffic.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab80696d-aa01-47f1-b178-fd4305c371ae",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# See what columns are common between all\ncols_svi = set(df_svi.columns)\ncols_hpsa = set(df_hpsa.columns)\ncols_noaa = set(df_noaa.columns)\ncols_traffic = set(df_traffic.columns)\n\ncommon_cols = cols_svi & cols_hpsa & cols_noaa\nprint(common_cols)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a1b3d58-e383-4255-867b-632109c78136",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "Join CDC, HPSA, and NOAA by FIPS "
  },
  {
   "cell_type": "code",
   "id": "daed6f9d-d9fd-40bf-9ab7-81f2a1f11253",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "-- First see common FIPS code between all\nWITH all_fips AS (\n    SELECT 'HPSA' as source, FIPS_CODE FROM CLEANED_HPSA\n    UNION ALL\n    SELECT 'CDC_SVI' as source, FIPS_CODE FROM CLEANED_CDC_SVI\n    UNION ALL\n    SELECT 'NOAA' as source, FIPS_CODE FROM CLEANED_NOAA\n),\ncounts AS (\n    SELECT \n        COUNT(DISTINCT CASE WHEN source = 'HPSA' THEN FIPS_CODE END) as total_hpsa,\n        COUNT(DISTINCT CASE WHEN source = 'CDC_SVI' THEN FIPS_CODE END) as total_cdc_svi,\n        COUNT(DISTINCT CASE WHEN source = 'NOAA' THEN FIPS_CODE END) as total_noaa\n    FROM all_fips\n)\nSELECT \n    total_hpsa,\n    total_cdc_svi,\n    total_noaa,\n    (SELECT COUNT(*) FROM (\n        SELECT FIPS_CODE FROM CLEANED_HPSA\n        INTERSECT\n        SELECT FIPS_CODE FROM CLEANED_CDC_SVI\n    )) as hpsa_cdc_common,\n    (SELECT COUNT(*) FROM (\n        SELECT FIPS_CODE FROM CLEANED_HPSA\n        INTERSECT\n        SELECT FIPS_CODE FROM CLEANED_NOAA\n    )) as hpsa_noaa_common,\n    (SELECT COUNT(*) FROM (\n        SELECT FIPS_CODE FROM CLEANED_CDC_SVI\n        INTERSECT\n        SELECT FIPS_CODE FROM CLEANED_NOAA\n    )) as cdc_noaa_common,\n    (SELECT COUNT(*) FROM (\n        SELECT FIPS_CODE FROM CLEANED_HPSA\n        INTERSECT\n        SELECT FIPS_CODE FROM CLEANED_CDC_SVI\n        INTERSECT\n        SELECT FIPS_CODE FROM CLEANED_NOAA\n    )) as all_three_common\nFROM counts;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eba84f86-8214-4d97-a817-acc863b4373a",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "Should have "
  },
  {
   "cell_type": "code",
   "id": "8b027f91-4e6a-4007-a3db-522f8d31c993",
   "metadata": {
    "language": "sql",
    "name": "cell15",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE JOINED_COUNTY_DATA AS\nSELECT \n    h.FIPS_CODE,\n    h.STATE_ABBR,\n    \n    -- HPSA Features (worst-case scenario)\n    MAX(h.HPSA_SCORE) as MAX_HPSA_SCORE,\n    AVG(h.HPSA_SCORE) as AVG_HPSA_SCORE,\n    COUNT(DISTINCT h.DESIGNATION_TYPE) as NUM_DESIGNATION_TYPES,\n    MAX(h.HPSA_DESIGNATION_POPULATION) as HPSA_POPULATION,\n    MAX(h.PCT_POPULATION_BELOW_POVERTY) as POVERTY_RATE,\n    MAX(h.RURAL_STATUS) as RURAL_STATUS,\n    \n    -- CDC SVI Features\n    MAX(c.OVERALL_SVI_SCORE) as SVI_SCORE,\n    MAX(c.SOCIOECONOMIC_VULNERABILITY) as SOCIOECONOMIC_SVI,\n    MAX(c.HOUSEHOLD_VULNERABILITY) as HOUSEHOLD_SVI,\n    MAX(c.MINORITY_STATUS) as MINORITY_SVI,\n    MAX(c.HOUSING_TRANSPORT) as HOUSING_TRANSPORT_SVI,\n    MAX(c.TOTAL_POPULATION) as TOTAL_POPULATION,\n    \n    -- NOAA Disaster Features\n    SUM(n.EVENT_COUNT) as TOTAL_DISASTER_EVENTS,\n    SUM(n.TOTAL_DAMAGE) as TOTAL_DAMAGE_COST,\n    COUNT(DISTINCT n.SEVERE_EVENT_TYPE) as NUM_DISASTER_TYPES\n\nFROM CLEANED_HPSA h\nINNER JOIN CLEANED_CDC_SVI c ON h.FIPS_CODE = c.FIPS_CODE\nINNER JOIN CLEANED_NOAA n ON h.FIPS_CODE = n.FIPS_CODE\nGROUP BY h.FIPS_CODE, h.STATE_ABBR;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddc5b898-94e9-4e29-9eb2-d887d761e0f0",
   "metadata": {
    "language": "sql",
    "name": "cell22",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n-- Should be ~787 counties",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d38bbc07-6bdb-4f49-9bb5-2ab2001119b9",
   "metadata": {
    "language": "sql",
    "name": "cell23",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM PROCESSED_DATA.JOINED_COUNTY_DATA LIMIT 10",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdb44539-39f3-4358-bd2f-679341a81bdc",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "Join by state with traffic"
  },
  {
   "cell_type": "code",
   "id": "a0a79819-1765-40d9-9459-dda39b738a9c",
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": "-- state level traffic\nCREATE OR REPLACE TABLE STATE_TRAFFIC_AVERAGES AS\nSELECT \n    STATE_ABBR,\n    AVG(TOTAL_CONGESTION_SCORE) as AVG_STATE_CONGESTION,\n    AVG(TOTAL_WEATHER_CONGESTION) as AVG_STATE_WEATHER_CONGESTION,\n    AVG(TOTAL_INCIDENT_CONGESTION) as AVG_STATE_INCIDENT_CONGESTION\nFROM CLEANED_TRAFFIC\nGROUP BY STATE_ABBR;\n\n\nCREATE OR REPLACE TABLE JOINED_COUNTY_DATA AS\nSELECT \n    h.FIPS_CODE,\n    h.STATE_ABBR,\n    \n    -- HPSA Features (worst-case scenario)\n    MAX(h.HPSA_SCORE) as MAX_HPSA_SCORE,\n    AVG(h.HPSA_SCORE) as AVG_HPSA_SCORE,\n    COUNT(DISTINCT h.DESIGNATION_TYPE) as NUM_DESIGNATION_TYPES,\n    MAX(h.HPSA_DESIGNATION_POPULATION) as HPSA_POPULATION,\n    MAX(h.PCT_POPULATION_BELOW_POVERTY) as POVERTY_RATE,\n    MAX(h.RURAL_STATUS) as RURAL_STATUS,\n    \n    -- CDC SVI Features\n    MAX(c.OVERALL_SVI_SCORE) as SVI_SCORE,\n    MAX(c.SOCIOECONOMIC_VULNERABILITY) as SOCIOECONOMIC_SVI,\n    MAX(c.HOUSEHOLD_VULNERABILITY) as HOUSEHOLD_SVI,\n    MAX(c.MINORITY_STATUS) as MINORITY_SVI,\n    MAX(c.HOUSING_TRANSPORT) as HOUSING_TRANSPORT_SVI,\n    MAX(c.TOTAL_POPULATION) as TOTAL_POPULATION,\n    \n    -- NOAA Disaster Features\n    SUM(n.EVENT_COUNT) as TOTAL_DISASTER_EVENTS,\n    SUM(n.TOTAL_DAMAGE) as TOTAL_DAMAGE_COST,\n    COUNT(DISTINCT n.SEVERE_EVENT_TYPE) as NUM_DISASTER_TYPES\n\nFROM CLEANED_HPSA h\nINNER JOIN CLEANED_CDC_SVI c ON h.FIPS_CODE = c.FIPS_CODE\nINNER JOIN CLEANED_NOAA n ON h.FIPS_CODE = n.FIPS_CODE\nGROUP BY h.FIPS_CODE, h.STATE_ABBR;\n\n\nCREATE OR REPLACE TABLE FINAL_MASTER_DATASET AS\nSELECT \n    j.*,\n    \n    -- State-level traffic features\n    t.AVG_STATE_CONGESTION as STATE_TRAFFIC_CONGESTION,\n    t.AVG_STATE_WEATHER_CONGESTION as STATE_WEATHER_TRAFFIC,\n    t.AVG_STATE_INCIDENT_CONGESTION as STATE_INCIDENT_TRAFFIC\n    \nFROM JOINED_COUNTY_DATA j\nLEFT JOIN STATE_TRAFFIC_AVERAGES t ON j.STATE_ABBR = t.STATE_ABBR;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab080916-3e6c-4860-9774-da9ad06bbee1",
   "metadata": {
    "language": "sql",
    "name": "cell29"
   },
   "outputs": [],
   "source": "SELECT * FROM PROCESSED_DATA.FINAL_MASTER_DATASET LIMIT 20",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4845ac13-6632-45df-9494-d94043a4d90d",
   "metadata": {
    "language": "sql",
    "name": "cell28",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Check the final dataset\nSELECT \n    COUNT(*) as total_counties,\n    COUNT(DISTINCT STATE_ABBR) as states_represented,\n    AVG(STATE_TRAFFIC_CONGESTION) as avg_traffic_score\nFROM FINAL_MASTER_DATASET;\n\n-- See traffic data by state\nSELECT \n    STATE_ABBR,\n    COUNT(*) as county_count,\n    AVG(STATE_TRAFFIC_CONGESTION) as state_traffic_avg\nFROM FINAL_MASTER_DATASET\nGROUP BY STATE_ABBR\nORDER BY state_traffic_avg DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef3cc9ea-af7c-41bb-bbb0-85b4ff8f54c4",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "Next steps: make final risk score/scores by normalizing\n\n\n-Medical Risk = HPSA Ã— SVI (county-level)\n-Disaster Risk = Storm events (county-level)  \n-Infrastructure Risk = Traffic congestion (state-level)\n-Overall Risk = Weighted combination of all three"
  },
  {
   "cell_type": "markdown",
   "id": "65359890-a7a0-48bd-b749-f31338b39a29",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "CREATING RISK SCORES + REGRESSION MODEL "
  },
  {
   "cell_type": "code",
   "id": "1477dfae-6b22-4dd9-a80f-9cc105dad995",
   "metadata": {
    "language": "python",
    "name": "Setup_and_Data_Loading",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ===== REGRESSION MODEL WITH STORM FEATURES =====\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col, coalesce, lit, lpad\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsession = get_active_session()\n\nprint(f\" Current Location: {session.get_current_database()}.{session.get_current_schema()}\")\n\n# ===== LOAD FINAL MASTER DATASET =====\nfinal_df = session.table(\"FINAL_MASTER_DATASET\")\nprint(f\" Loaded FINAL_MASTER_DATASET: {final_df.count()} counties\")\n\n# ===== LOAD STORM FEATURES FROM STORM MODEL =====\nstorm_features = session.table(\"NOAA_STORM_EVENTS_2025.PUBLIC.STORM_COUNTY_RISK_FEATURES\")\nprint(f\" Loaded STORM_COUNTY_RISK_FEATURES: {storm_features.count()} counties\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7ce2b36-a425-4ed8-b288-df539891a08a",
   "metadata": {
    "language": "python",
    "name": "Join_Storm_Features"
   },
   "outputs": [],
   "source": "# ===== ENSURE BOTH FIPS CODES ARE 5-DIGIT STRINGS =====\n# clean final dataset FIPS\nfinal_df = final_df.with_column(\n    \"FIPS_CODE_CLEAN\",\n    lpad(col(\"FIPS_CODE\").cast(\"STRING\"), lit(5), lit(\"0\")) # ensure all codes are 5 digits with leading zeros\n)\n\n# clean storm features FIPS \nstorm_features = storm_features.with_column(\n    \"FIPS_CODE_CLEAN\",\n    lpad(col(\"FIPS_CODE\").cast(\"STRING\"), lit(5), lit(\"0\"))\n)\n\n# ===== JOIN ON 5-DIGIT FIPS =====\nregression_df = final_df.join( # join Storm features \n    storm_features,\n    final_df[\"FIPS_CODE_CLEAN\"] == storm_features[\"FIPS_CODE_CLEAN\"],\n    join_type=\"left\"\n).drop(storm_features[\"FIPS_CODE\"], storm_features[\"FIPS_CODE_CLEAN\"])\n\nprint(\"Joined storm features at county level\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "baebb6a7-ccd5-4ec6-9942-2648e3bbd3d6",
   "metadata": {
    "language": "python",
    "name": "Handle_Missing_Data"
   },
   "outputs": [],
   "source": "# ===== FILL NULL VALUES FOR COUNTIES WITHOUT STORM DATA =====\nstorm_cols = [\n    'STORM_SEVERITY_MEAN',\n    'STORM_SEVERITY_MAX',\n    'STORM_SEVERITY_STD',\n    'STORM_INJURIES_TOTAL',\n    'STORM_DEATHS_TOTAL',\n    'STORM_DAMAGE_PROPERTY_TOTAL',\n    'STORM_DAMAGE_CROPS_TOTAL',\n    'STORM_EVENT_COUNT'\n]\n# for columns without storm data, the storm columns will be NULL\nfor col_name in storm_cols:\n    regression_df = regression_df.with_column(\n        col_name,\n        coalesce(col(col_name), lit(0.0))\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce477bbb-75e4-47a5-8b28-25260a0a79f0",
   "metadata": {
    "language": "python",
    "name": "More_data_handling"
   },
   "outputs": [],
   "source": "# ===== CONVERT TO PANDAS =====\nregression_pandas = regression_df.to_pandas()\nprint(f\" Converted to pandas. Shape: {regression_pandas.shape}\")\n\n# identify the FIPS column (it has a weird prefix from the join)\nfips_col = [c for c in regression_pandas.columns if 'FIPS_CODE' in c and 'CLEAN' not in c][0]\nprint(f\"\\n FIPS column identified as: {fips_col}\")\n\n# rename it to something simple\nregression_pandas = regression_pandas.rename(columns={fips_col: 'FIPS_CODE'})\n\n# check storm data coverage\ncounties_with_storms = regression_pandas[regression_pandas['STORM_EVENT_COUNT'] > 0].shape[0]\ntotal_counties = len(regression_pandas)\nprint(f\"\\n {counties_with_storms}/{total_counties} counties have storm history ({counties_with_storms/total_counties*100:.1f}%)\")\n\n# shows combined data \nprint(\"\\nðŸ“‹ Sample of data with storm features:\")\ndisplay_cols = ['FIPS_CODE', 'STATE_ABBR', 'STORM_EVENT_COUNT', \n                'STORM_SEVERITY_MEAN', 'STORM_DAMAGE_PROPERTY_TOTAL']\nprint(regression_pandas[display_cols].head(10))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acd6f07a-bc5c-43d9-b7c4-86bf35be5631",
   "metadata": {
    "language": "python",
    "name": "Calculate_Risk_Scores"
   },
   "outputs": [],
   "source": "# ===== CREATE SEPARATE RISK SCORES =====\n\n# ----- 1. MEDICAL RISK = HPSA Ã— SVI -----\n# Normalize each component first\nscaler_medical = MinMaxScaler()\nmedical_components = regression_pandas[['MAX_HPSA_SCORE', 'SVI_SCORE']].fillna(0)\nmedical_normalized = scaler_medical.fit_transform(medical_components)\n\n# Multiply HPSA Ã— SVI (both normalized to 0-1)\nregression_pandas['MEDICAL_RISK'] = (\n    medical_normalized[:, 0] * medical_normalized[:, 1]\n) * 100  # Scale to 0-100\n\nprint(f\" Medical Risk created (HPSA Ã— SVI)\")\nprint(f\"   Range: {regression_pandas['MEDICAL_RISK'].min():.2f} - {regression_pandas['MEDICAL_RISK'].max():.2f}\")\nprint(f\"   Mean: {regression_pandas['MEDICAL_RISK'].mean():.2f}\")\n\n# ----- 2. DISASTER RISK = Storm Model Predictions -----\n# Use our storm features to create disaster risk\nscaler_disaster = MinMaxScaler()\ndisaster_components = regression_pandas[[\n    'STORM_SEVERITY_MEAN',\n    'STORM_SEVERITY_MAX', \n    'STORM_EVENT_COUNT',\n    'STORM_DAMAGE_PROPERTY_TOTAL',\n    'STORM_INJURIES_TOTAL',\n    'STORM_DEATHS_TOTAL'\n]].fillna(0)\ndisaster_normalized = scaler_disaster.fit_transform(disaster_components)\n\n# Weighted combination of storm features\ndisaster_weights = {\n    'STORM_SEVERITY_MEAN': 0.30,           # How severe on average\n    'STORM_SEVERITY_MAX': 0.20,            # Worst case scenario\n    'STORM_EVENT_COUNT': 0.15,             # Frequency\n    'STORM_DAMAGE_PROPERTY_TOTAL': 0.20,   # Economic impact\n    'STORM_INJURIES_TOTAL': 0.10,          # Human impact\n    'STORM_DEATHS_TOTAL': 0.05             # Fatality risk\n}\n\n# multiply each normalized storm feature by its weight\n# add them all together, scale to 0-100\nregression_pandas['DISASTER_RISK'] = (\n    disaster_normalized[:, 0] * disaster_weights['STORM_SEVERITY_MEAN'] +\n    disaster_normalized[:, 1] * disaster_weights['STORM_SEVERITY_MAX'] +\n    disaster_normalized[:, 2] * disaster_weights['STORM_EVENT_COUNT'] +\n    disaster_normalized[:, 3] * disaster_weights['STORM_DAMAGE_PROPERTY_TOTAL'] +\n    disaster_normalized[:, 4] * disaster_weights['STORM_INJURIES_TOTAL'] +\n    disaster_normalized[:, 5] * disaster_weights['STORM_DEATHS_TOTAL']\n) * 100  # Scale to 0-100\n\nprint(f\"\\n Disaster Risk created (Storm Model Features)\")\nprint(f\"   Range: {regression_pandas['DISASTER_RISK'].min():.2f} - {regression_pandas['DISASTER_RISK'].max():.2f}\")\nprint(f\"   Mean: {regression_pandas['DISASTER_RISK'].mean():.2f}\")\n\n# ----- 3. INFRASTRUCTURE RISK = Traffic Congestion -----\n# takes state-level traffic congestion score\n# normalizes to 0-1\n# scales to 0-100\nscaler_infra = MinMaxScaler()\ninfra_components = regression_pandas[['STATE_TRAFFIC_CONGESTION']].fillna(0)\ninfra_normalized = scaler_infra.fit_transform(infra_components)\n\nregression_pandas['INFRASTRUCTURE_RISK'] = infra_normalized[:, 0] * 100\n\nprint(f\"\\n Infrastructure Risk created (Traffic Congestion)\")\nprint(f\"   Range: {regression_pandas['INFRASTRUCTURE_RISK'].min():.2f} - {regression_pandas['INFRASTRUCTURE_RISK'].max():.2f}\")\nprint(f\"   Mean: {regression_pandas['INFRASTRUCTURE_RISK'].mean():.2f}\")\n\n# ----- 4. OVERALL RISK = Weighted Combination -----(\n# current weights (ADJUST IF NECESSARY)\noverall_weights = {\n    'MEDICAL_RISK': 0.40,           # 40% weight\n    'DISASTER_RISK': 0.35,          # 35% weight\n    'INFRASTRUCTURE_RISK': 0.25     # 25% weight\n}\n\nregression_pandas['OVERALL_RISK'] = (\n    (regression_pandas['MEDICAL_RISK'] / 100) * overall_weights['MEDICAL_RISK'] +\n    (regression_pandas['DISASTER_RISK'] / 100) * overall_weights['DISASTER_RISK'] +\n    (regression_pandas['INFRASTRUCTURE_RISK'] / 100) * overall_weights['INFRASTRUCTURE_RISK']\n) * 100\n\nprint(f\"\\n Overall Risk created (Weighted Combination)\")\nprint(f\"   Weights: Medical {overall_weights['MEDICAL_RISK']:.0%}, Disaster {overall_weights['DISASTER_RISK']:.0%}, Infrastructure {overall_weights['INFRASTRUCTURE_RISK']:.0%}\")\nprint(f\"   Range: {regression_pandas['OVERALL_RISK'].min():.2f} - {regression_pandas['OVERALL_RISK'].max():.2f}\")\nprint(f\"   Mean: {regression_pandas['OVERALL_RISK'].mean():.2f}\")\n\n# ----- 5. SHOW DISTRIBUTION OF RISK SCORES -----\nprint(\"\\n RISK SCORE DISTRIBUTIONS:\")\nprint(\"\\nMedical Risk:\")\nprint(regression_pandas['MEDICAL_RISK'].describe())\nprint(\"\\nDisaster Risk:\")\nprint(regression_pandas['DISASTER_RISK'].describe())\nprint(\"\\nInfrastructure Risk:\")\nprint(regression_pandas['INFRASTRUCTURE_RISK'].describe())\nprint(\"\\nOverall Risk:\")\nprint(regression_pandas['OVERALL_RISK'].describe())\n\n# ----- 6. SHOW SAMPLE COUNTIES -----\nprint(\"\\n SAMPLE: Risk Breakdown by County\")\nsample_cols = ['FIPS_CODE', 'STATE_ABBR', 'MEDICAL_RISK', 'DISASTER_RISK', \n               'INFRASTRUCTURE_RISK', 'OVERALL_RISK']\nprint(regression_pandas[sample_cols].head(10).round(2).to_string(index=False))\n\nprint(\"=\"*60 + \"\\n\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cd66dd5-f706-40a6-a624-95204d858aa8",
   "metadata": {
    "name": "EDA",
    "collapsed": false
   },
   "source": "Add some Exploratory Data Analysis"
  },
  {
   "cell_type": "code",
   "id": "6a860b0c-c7dc-4275-a22c-6b5c99334523",
   "metadata": {
    "language": "python",
    "name": "summarystats"
   },
   "outputs": [],
   "source": "print(\"\\nBasic Statistics:\")\ndisplay(regression_pandas.describe())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1021e8f0-6910-4c75-99f5-0c0b81fe4875",
   "metadata": {
    "language": "python",
    "name": "corrMatrix"
   },
   "outputs": [],
   "source": "# Manually select the key numeric features you want to correlate\nkey_numeric_features = [\n    'MAX_HPSA_SCORE', 'AVG_HPSA_SCORE', 'SVI_SCORE', 'SOCIOECONOMIC_SVI',\n    'HOUSEHOLD_SVI', 'MINORITY_SVI', 'HOUSING_TRANSPORT_SVI', 'POVERTY_RATE',\n    'TOTAL_POPULATION', 'TOTAL_DISASTER_EVENTS', 'TOTAL_DAMAGE_COST', \n    'NUM_DISASTER_TYPES', 'STATE_TRAFFIC_CONGESTION', 'STATE_WEATHER_TRAFFIC',\n    'STATE_INCIDENT_TRAFFIC'\n]\n\n# Filter to only include columns that actually exist in your dataframe\nexisting_features = [col for col in key_numeric_features if col in regression_pandas.columns]\n\nplt.figure(figsize=(14, 10))\ncorr_matrix = regression_pandas[existing_features].corr()\n\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n            center=0, square=True, cbar_kws={\"shrink\": .8})\nplt.title('Key Features Correlation Matrix', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2153e775-7ebf-4f91-bac8-9f67fc25f215",
   "metadata": {
    "language": "python",
    "name": "Prepared_For_Modeling"
   },
   "outputs": [],
   "source": "# ===== PREPARE FEATURES FOR MODELING =====\n# remove what we are trying to predict, so we don't use them as inputs\nexclude_cols = [\n    'FIPS_CODE', \n    'MEDICAL_RISK', 'DISASTER_RISK', 'INFRASTRUCTURE_RISK', 'OVERALL_RISK',  # These are targets/outputs\n    'STATE_ABBR', 'RURAL_STATUS'\n]\n\n\nfeature_cols = [col for col in regression_pandas.columns \n                if col not in exclude_cols and regression_pandas[col].dtype in ['int64', 'float64']]\n\n# set features and label\n# what we are predicting: overall risk from the features \nX = regression_pandas[feature_cols].fillna(0)\ny = regression_pandas['OVERALL_RISK']  # â† Changed from RISK_SCORE to OVERALL_RISK\n\nprint(f\"\\nðŸ“‹ Features in model:\")\nprint(f\"   Total features: {len(feature_cols)}\")\nprint(f\"   Storm features: {len([c for c in feature_cols if c.startswith('STORM_')])}\")\nprint(f\"   Target: OVERALL_RISK (combination of Medical, Disaster, Infrastructure)\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d8f8689-03ca-47f8-9533-45f5331438e2",
   "metadata": {
    "language": "python",
    "name": "Train_and_Evaluate_Model"
   },
   "outputs": [],
   "source": "# ===== TRAIN_TEST_SPLIT =====\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"\\n Data split:\")\nprint(f\"   Training samples: {len(X_train)}\")\nprint(f\"   Testing samples: {len(X_test)}\")\n\n# ===== SCALE FEATURES =====\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# ===== TRAIN MODEL =====\nprint(\"\\n Training Gradient Boosting Regressor...\")\n\nreg_model = GradientBoostingRegressor(\n    n_estimators=100,      # Keep\n    max_depth=3,           # â† Reduce to 3 (from 4)\n    learning_rate=0.1,     # Keep\n    random_state=42,\n    subsample=0.8,\n    min_samples_split=15,  # â† Increase to 15 (from 10)\n    min_samples_leaf=7     # â† Increase to 7 (from 5)\n)\n\nreg_model.fit(X_train_scaled, y_train)\nprint(\" Model training complete!\")\n\n# ===== EVALUATE MODEL =====\ny_pred_train = reg_model.predict(X_train_scaled)\ny_pred_test = reg_model.predict(X_test_scaled)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\" REGRESSION MODEL PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"Train RÂ² Score: {r2_score(y_train, y_pred_train):.4f}\")\nprint(f\"Test RÂ² Score:  {r2_score(y_test, y_pred_test):.4f}\")\nprint(f\"Test RMSE:      {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\nprint(f\"Test MAE:       {mean_absolute_error(y_test, y_pred_test):.4f}\")\nprint(\"=\"*60)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32666b57-213d-405e-92bc-31babd7050fe",
   "metadata": {
    "language": "python",
    "name": "Evaluate_Feature_Importance"
   },
   "outputs": [],
   "source": "# ===== FEATURE IMPORTANCE =====\nfeature_importance = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': reg_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\n TOP 20 MOST IMPORTANT FEATURES:\")\nprint(feature_importance.head(20).to_string(index=False))\n\n# highlight storm features\nstorm_feature_importance = feature_importance[\n    feature_importance['feature'].str.startswith('STORM_')\n].reset_index(drop=True)\n\nif len(storm_feature_importance) > 0:\n    print(\"\\n STORM FEATURE IMPORTANCE (from prediction model):\")\n    print(storm_feature_importance.to_string(index=False))\n    \n    total_storm_importance = storm_feature_importance['importance'].sum()\n    print(f\"\\n Storm features contribute {total_storm_importance:.2%} to overall model importance\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62081903-c0f9-4e6b-8186-b85720a0b980",
   "metadata": {
    "language": "python",
    "name": "Visualizations",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ===== VISUALIZATIONS =====\n# Plot 1: Feature Importance\nplt.figure(figsize=(12, 8))\ntop_20 = feature_importance.head(20)\ncolors = ['red' if x.startswith('STORM_') else 'steelblue' for x in top_20['feature']]\nplt.barh(range(len(top_20)), top_20['importance'], color=colors)\nplt.yticks(range(len(top_20)), top_20['feature'])\nplt.xlabel('Importance Score')\nplt.title('Top 20 Most Important Features\\n(Red = Storm Model Features)', fontsize=14, fontweight='bold')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n# Plot 2: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k', s=50)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\nplt.xlabel('Actual Risk Score', fontsize=12)\nplt.ylabel('Predicted Risk Score', fontsize=12)\nplt.title(f'Actual vs Predicted Risk Scores\\nRÂ² = {r2_score(y_test, y_pred_test):.4f}', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Plot 3: Residuals\nplt.figure(figsize=(10, 6))\nresiduals = y_test - y_pred_test\nplt.scatter(y_pred_test, residuals, alpha=0.6, edgecolors='k', s=50)\nplt.axhline(y=0, color='r', linestyle='--', lw=2, label='Zero Residual')\nplt.xlabel('Predicted Risk Score', fontsize=12)\nplt.ylabel('Residuals', fontsize=12)\nplt.title('Residual Plot', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf319f55-9118-4c5f-a874-b826f1d59946",
   "metadata": {
    "language": "python",
    "name": "Save_Predictions"
   },
   "outputs": [],
   "source": "# ===== SAVE PREDICTIONS BACK TO SNOWFLAKE =====\nregression_pandas['PREDICTED_OVERALL_RISK'] = reg_model.predict(scaler.transform(X))\n\n# create output dataframe with ALL risk scores\noutput_df = regression_pandas[[\n    'FIPS_CODE', \n    'STATE_ABBR', \n    'MEDICAL_RISK',\n    'DISASTER_RISK', \n    'INFRASTRUCTURE_RISK',\n    'OVERALL_RISK',\n    'PREDICTED_OVERALL_RISK'\n] + storm_cols].copy()\n\n# Convert back to Snowpark and save\noutput_sp = session.create_dataframe(output_df)\noutput_sp.write.mode(\"overwrite\").save_as_table(\"COUNTY_RISK_PREDICTIONS_WITH_STORM\")\n\nprint(\"\\n Predictions saved to COUNTY_RISK_PREDICTIONS_WITH_STORM table\")\nprint(f\" Saved {len(output_df)} county risk predictions\")\nprint(\"\\n Output includes:\")\nprint(\"   - MEDICAL_RISK (HPSA Ã— SVI)\")\nprint(\"   - DISASTER_RISK (Storm model predictions)\")\nprint(\"   - INFRASTRUCTURE_RISK (Traffic congestion)\")\nprint(\"   - OVERALL_RISK (Weighted combination)\")\nprint(\"   - PREDICTED_OVERALL_RISK (ML model prediction)\")\nprint(\"   - All 8 storm features\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec0b2d2f-87ba-4edc-a3d5-418b7d24e982",
   "metadata": {
    "language": "python",
    "name": "Final_Summary"
   },
   "outputs": [],
   "source": "# ===== FINAL SUMMARY =====\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Total counties analyzed: {total_counties}\")\nprint(f\"Counties with storm history: {counties_with_storms} ({counties_with_storms/total_counties*100:.1f}%)\")\nprint(f\"\\nRisk Score Breakdown:\")\nprint(f\"   Medical Risk (HPSA Ã— SVI): {regression_pandas['MEDICAL_RISK'].mean():.2f} avg\")\nprint(f\"   Disaster Risk (Storm Model): {regression_pandas['DISASTER_RISK'].mean():.2f} avg\")\nprint(f\"   Infrastructure Risk (Traffic): {regression_pandas['INFRASTRUCTURE_RISK'].mean():.2f} avg\")\nprint(f\"   Overall Risk (Combined): {regression_pandas['OVERALL_RISK'].mean():.2f} avg\")\n\n# Show top 10 highest risk counties (for stakeholders)\nprint(\"\\n TOP 10 HIGHEST RISK COUNTIES:\")\ntop_risk_cols = ['FIPS_CODE', 'STATE_ABBR', 'OVERALL_RISK', 'MEDICAL_RISK', \n                 'DISASTER_RISK', 'INFRASTRUCTURE_RISK']\ntop_risk = regression_pandas.nlargest(10, 'OVERALL_RISK')[top_risk_cols]\nprint(top_risk.round(2).to_string(index=False))\n\nprint(\"\\n Analysis complete!\")",
   "execution_count": null
  }
 ]
}